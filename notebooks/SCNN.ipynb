{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow imports\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, MaxPooling2D, Dense, Input, Activation, AveragePooling2D, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, GlorotNormal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from scipy import signal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "from matplotlib import pyplot as plt\n",
    "from pywt._doc_utils import wavedec2_keys, draw_2d_wp_basis\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a filter\n",
    "filter1 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [-1,-2,-3], [1,2,3]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "filtera1 = rotate(filter1, angle=90)\n",
    "filtera2 = rotate(filter1, angle=180)\n",
    "filtera3 = rotate(filter1, angle=270)\n",
    "\n",
    "\n",
    "# d filter\n",
    "filter2 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [-1,-2,-3],[2,4,6],[-1,-2,-3], [0,0,0]],\n",
    "                    [[0,0,0], [2,4,6],[-4,-8,-12], [2,4,6],[0,0,0]],\n",
    "                    [[0,0,0], [-1,-2,-3], [2,4,6], [-1,-2,-3], [0,0,0]],\n",
    "                    [[0,0,0],[0,0,0],[0,0,0], [0,0,0], [0,0,0]]])\n",
    "\n",
    "\n",
    "# b filter\n",
    "fitler3 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [1,2,3], [-2,-4,-6], [1,2,3], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "filterb1 =  signal.convolve(filter2, filter2, mode='same')\n",
    "filterb2 =  signal.convolve(filter2, filter2, mode='same')\n",
    "filterb3 =  signal.convolve(filter2, filter2, mode='same')\n",
    "\n",
    "# c filter\n",
    "filter4 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]], [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]], [\n",
    "    [0,0,0], [1,2,3],[-3,-6,-9],[3,6,9],[-1,-2,-3]], [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "\n",
    "#this is not filter\n",
    "filter4RDT = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [1,2,3], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [-3,-6,-9], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [3,6,9], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [-1,-2,-3], [0,0,0], [0,0,0]]])\n",
    "\n",
    "\n",
    "filterc1 =  signal.convolve(filter4, filter4, mode='same')\n",
    "filterc2 =  signal.convolve(filter4.T, filter4.T, mode='same').reshape(5,5,3)\n",
    "filterc3 =  signal.convolve(filter4RDT, filter4RDT, mode='same').reshape(5,5,3)\n",
    "\n",
    "#e filter\n",
    "filter5 = np.array([[[-1,-2,-3], [2,4,6], [-2,-4,-6], [2,4,6], [-1,-2,-3]],\n",
    "                    [[2,4,6], [-6,-12,-18], [-8,-16,-24], [-6,-12,-18], [2,4,6]],\n",
    "                   [[-2,-4,-6], [8,16,24], [-12,-24,-36], [8,16,24], [2,4,6]],\n",
    "                    [[2,4,6], [-6,-12,-18], [8,16,24], [-6,-12,-18], [2,4,6]],\n",
    "                    [[-1,-2,-3], [2,4,6], [-2,-4,-6], [2,4,6], [-1,-2,-3]]])\n",
    "\n",
    "filter_list = [filter1.tolist(), filtera1.tolist(), filtera2.tolist(), filtera3.tolist(), filter2.tolist(),\\\n",
    "               fitler3.tolist(), filterb1.tolist(), filterb2.tolist(), filterb3.tolist(),\\\n",
    "               filter4.tolist(), filterc1.tolist(), filterc2.tolist(), filterc3.tolist(), filter5.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 5, 5, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([filter1,filtera1, filtera2, filtera3, filter2, fitler3, filterb1, filterb2, filterb3, filter4, filterc1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 14)        1064      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 16)        5616      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 62, 62, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 62, 62, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 62, 62, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 31, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 81,593\n",
      "Trainable params: 80,601\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inp = Input(shape = (64, 64, 3))\n",
    "x = Conv2D(14, (5,5), padding = 'same', name = 'conv2d_1')(Inp)\n",
    "x = Conv2D(16, (5,5), kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_2')(x)\n",
    "x = ZeroPadding2D((1,1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(32, (5,5), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(128, (1,1), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_5')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(256, (1,1), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_6')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = AveragePooling2D((2,2))(x)\n",
    "x = Flatten()(x)\n",
    "out = Dense(1, activation = 'sigmoid',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'ouput')(x)\n",
    "\n",
    "model = Model(inputs = [Inp], outputs = [out])\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, show_shapes =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/logs')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/weights')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/checkpoints')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(\"../models/weights/tanh_history_log2.csv\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "log_dir = r\"..\\\\models\\\\logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filepath = '../models/checkpoints/model-tanh-try2.h5';\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('conv2d_1')\n",
    "layer.set_weights([np.array(filter_list).astype(np.float32).reshape(5,5,3,14), np.zeros(14,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3751091 images belonging to 2 classes.\n",
      "Found 537266 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.0,shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../input/patches/train/',  # This is the source directory for training images\n",
    "        target_size=(64, 64),  # All images will be resized to 64x64\n",
    "        batch_size=64,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '../input/patches/validation/',  # This is the source directory for validation images\n",
    "        target_size=(64, 64),  # All images will be resized to 64x64\n",
    "        batch_size=64,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 0, 'pristine': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 58611 steps, validate for 8395 steps\n",
      "Epoch 1/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.4955 - accuracy: 0.7620\n",
      "Epoch 00001: val_loss improved from inf to 0.47806, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5754s 98ms/step - loss: 0.4955 - accuracy: 0.7620 - val_loss: 0.4781 - val_accuracy: 0.7610\n",
      "Epoch 2/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.8016\n",
      "Epoch 00002: val_loss improved from 0.47806 to 0.41553, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5780s 99ms/step - loss: 0.4303 - accuracy: 0.8016 - val_loss: 0.4155 - val_accuracy: 0.8083\n",
      "Epoch 3/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.8162\n",
      "Epoch 00003: val_loss improved from 0.41553 to 0.39746, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5778s 99ms/step - loss: 0.4038 - accuracy: 0.8162 - val_loss: 0.3975 - val_accuracy: 0.8216\n",
      "Epoch 4/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.8255\n",
      "Epoch 00004: val_loss did not improve from 0.39746\n",
      "58611/58611 [==============================] - 5754s 98ms/step - loss: 0.3867 - accuracy: 0.8255 - val_loss: 0.4059 - val_accuracy: 0.8165\n",
      "Epoch 5/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8322\n",
      "Epoch 00005: val_loss did not improve from 0.39746\n",
      "58611/58611 [==============================] - 5805s 99ms/step - loss: 0.3737 - accuracy: 0.8322 - val_loss: 0.4470 - val_accuracy: 0.8065\n",
      "Epoch 6/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8380 ETA: 0s - loss: 0.3632 - accuracy\n",
      "Epoch 00006: val_loss improved from 0.39746 to 0.38955, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5888s 100ms/step - loss: 0.3632 - accuracy: 0.8380 - val_loss: 0.3895 - val_accuracy: 0.8222\n",
      "Epoch 7/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8422\n",
      "Epoch 00007: val_loss did not improve from 0.38955\n",
      "58611/58611 [==============================] - 5942s 101ms/step - loss: 0.3550 - accuracy: 0.8422 - val_loss: 0.4044 - val_accuracy: 0.8192\n",
      "Epoch 8/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.8462\n",
      "Epoch 00008: val_loss improved from 0.38955 to 0.35821, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5804s 99ms/step - loss: 0.3474 - accuracy: 0.8462 - val_loss: 0.3582 - val_accuracy: 0.8397\n",
      "Epoch 9/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.8494\n",
      "Epoch 00009: val_loss did not improve from 0.35821\n",
      "58611/58611 [==============================] - 5864s 100ms/step - loss: 0.3411 - accuracy: 0.8494 - val_loss: 0.4028 - val_accuracy: 0.8198\n",
      "Epoch 10/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8523\n",
      "Epoch 00010: val_loss improved from 0.35821 to 0.34490, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5854s 100ms/step - loss: 0.3357 - accuracy: 0.8523 - val_loss: 0.3449 - val_accuracy: 0.8493\n",
      "Epoch 11/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8545\n",
      "Epoch 00011: val_loss did not improve from 0.34490\n",
      "58611/58611 [==============================] - 5895s 101ms/step - loss: 0.3310 - accuracy: 0.8545 - val_loss: 0.3782 - val_accuracy: 0.8303\n",
      "Epoch 12/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8568\n",
      "Epoch 00012: val_loss did not improve from 0.34490\n",
      "58611/58611 [==============================] - 5845s 100ms/step - loss: 0.3266 - accuracy: 0.8568 - val_loss: 0.4078 - val_accuracy: 0.8196\n",
      "Epoch 13/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8587\n",
      "Epoch 00013: val_loss did not improve from 0.34490\n",
      "58611/58611 [==============================] - 5779s 99ms/step - loss: 0.3225 - accuracy: 0.8587 - val_loss: 0.3554 - val_accuracy: 0.8441\n",
      "Epoch 14/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8605\n",
      "Epoch 00014: val_loss did not improve from 0.34490\n",
      "58611/58611 [==============================] - 5786s 99ms/step - loss: 0.3191 - accuracy: 0.8605 - val_loss: 0.3575 - val_accuracy: 0.8435\n",
      "Epoch 15/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8620\n",
      "Epoch 00015: val_loss improved from 0.34490 to 0.33904, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5771s 98ms/step - loss: 0.3158 - accuracy: 0.8620 - val_loss: 0.3390 - val_accuracy: 0.8541\n",
      "Epoch 16/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8632\n",
      "Epoch 00016: val_loss did not improve from 0.33904\n",
      "58611/58611 [==============================] - 5804s 99ms/step - loss: 0.3135 - accuracy: 0.8632 - val_loss: 0.3829 - val_accuracy: 0.8320\n",
      "Epoch 17/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8649\n",
      "Epoch 00017: val_loss improved from 0.33904 to 0.32700, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5772s 98ms/step - loss: 0.3105 - accuracy: 0.8649 - val_loss: 0.3270 - val_accuracy: 0.8594\n",
      "Epoch 18/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8655\n",
      "Epoch 00018: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5787s 99ms/step - loss: 0.3087 - accuracy: 0.8655 - val_loss: 0.3323 - val_accuracy: 0.8570\n",
      "Epoch 19/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8667\n",
      "Epoch 00019: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5793s 99ms/step - loss: 0.3066 - accuracy: 0.8667 - val_loss: 0.3552 - val_accuracy: 0.8485\n",
      "Epoch 20/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8678\n",
      "Epoch 00020: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5825s 99ms/step - loss: 0.3044 - accuracy: 0.8678 - val_loss: 0.3617 - val_accuracy: 0.8425\n",
      "Epoch 21/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8685 ETA: 0s - loss: 0.3027 - accuracy: \n",
      "Epoch 00021: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5779s 99ms/step - loss: 0.3027 - accuracy: 0.8685 - val_loss: 0.3386 - val_accuracy: 0.8535\n",
      "Epoch 22/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8692\n",
      "Epoch 00022: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5800s 99ms/step - loss: 0.3010 - accuracy: 0.8692 - val_loss: 0.3418 - val_accuracy: 0.8533\n",
      "Epoch 23/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8701\n",
      "Epoch 00023: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5765s 98ms/step - loss: 0.2997 - accuracy: 0.8701 - val_loss: 0.3641 - val_accuracy: 0.8430\n",
      "Epoch 24/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8710\n",
      "Epoch 00024: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5744s 98ms/step - loss: 0.2982 - accuracy: 0.8710 - val_loss: 0.3314 - val_accuracy: 0.8583\n",
      "Epoch 25/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8715\n",
      "Epoch 00025: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5944s 101ms/step - loss: 0.2967 - accuracy: 0.8715 - val_loss: 0.3308 - val_accuracy: 0.8587\n",
      "Epoch 26/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8722\n",
      "Epoch 00026: val_loss did not improve from 0.32700\n",
      "58611/58611 [==============================] - 5863s 100ms/step - loss: 0.2954 - accuracy: 0.8722 - val_loss: 0.3331 - val_accuracy: 0.8588\n",
      "Epoch 27/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8729\n",
      "Epoch 00027: val_loss improved from 0.32700 to 0.32299, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5885s 100ms/step - loss: 0.2942 - accuracy: 0.8729 - val_loss: 0.3230 - val_accuracy: 0.8614\n",
      "Epoch 28/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8734\n",
      "Epoch 00028: val_loss did not improve from 0.32299\n",
      "58611/58611 [==============================] - 5892s 101ms/step - loss: 0.2927 - accuracy: 0.8734 - val_loss: 0.3264 - val_accuracy: 0.8612\n",
      "Epoch 29/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8741\n",
      "Epoch 00029: val_loss improved from 0.32299 to 0.32260, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5912s 101ms/step - loss: 0.2914 - accuracy: 0.8741 - val_loss: 0.3226 - val_accuracy: 0.8620\n",
      "Epoch 30/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8746\n",
      "Epoch 00030: val_loss improved from 0.32260 to 0.31960, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5839s 100ms/step - loss: 0.2908 - accuracy: 0.8746 - val_loss: 0.3196 - val_accuracy: 0.8637\n",
      "Epoch 31/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8753\n",
      "Epoch 00031: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5871s 100ms/step - loss: 0.2894 - accuracy: 0.8753 - val_loss: 0.3249 - val_accuracy: 0.8612\n",
      "Epoch 32/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.8758 E\n",
      "Epoch 00032: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5875s 100ms/step - loss: 0.2883 - accuracy: 0.8758 - val_loss: 0.3462 - val_accuracy: 0.8520\n",
      "Epoch 33/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8763\n",
      "Epoch 00033: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5849s 100ms/step - loss: 0.2875 - accuracy: 0.8763 - val_loss: 0.3302 - val_accuracy: 0.8602\n",
      "Epoch 34/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.8766\n",
      "Epoch 00034: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5910s 101ms/step - loss: 0.2865 - accuracy: 0.8766 - val_loss: 0.3196 - val_accuracy: 0.8641\n",
      "Epoch 35/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8768\n",
      "Epoch 00035: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5909s 101ms/step - loss: 0.2859 - accuracy: 0.8768 - val_loss: 0.3458 - val_accuracy: 0.8514\n",
      "Epoch 36/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8777\n",
      "Epoch 00036: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5875s 100ms/step - loss: 0.2846 - accuracy: 0.8777 - val_loss: 0.3659 - val_accuracy: 0.8465\n",
      "Epoch 37/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8779\n",
      "Epoch 00037: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5878s 100ms/step - loss: 0.2839 - accuracy: 0.8779 - val_loss: 0.3349 - val_accuracy: 0.8584\n",
      "Epoch 38/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.8781\n",
      "Epoch 00038: val_loss did not improve from 0.31960\n",
      "58611/58611 [==============================] - 5874s 100ms/step - loss: 0.2835 - accuracy: 0.8781 - val_loss: 0.3246 - val_accuracy: 0.8622\n",
      "Epoch 39/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8787\n",
      "Epoch 00039: val_loss improved from 0.31960 to 0.31421, saving model to ../models/checkpoints/model-tanh-try2.h5\n",
      "58611/58611 [==============================] - 5889s 100ms/step - loss: 0.2822 - accuracy: 0.8787 - val_loss: 0.3142 - val_accuracy: 0.8660\n",
      "Epoch 40/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8791\n",
      "Epoch 00040: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5928s 101ms/step - loss: 0.2815 - accuracy: 0.8791 - val_loss: 0.3265 - val_accuracy: 0.8600\n",
      "Epoch 41/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2804 - accuracy: 0.8797\n",
      "Epoch 00041: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5900s 101ms/step - loss: 0.2804 - accuracy: 0.8797 - val_loss: 0.3390 - val_accuracy: 0.8541\n",
      "Epoch 42/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.8797\n",
      "Epoch 00042: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5877s 100ms/step - loss: 0.2798 - accuracy: 0.8797 - val_loss: 0.3220 - val_accuracy: 0.8630\n",
      "Epoch 43/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8802\n",
      "Epoch 00043: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5837s 100ms/step - loss: 0.2793 - accuracy: 0.8802 - val_loss: 0.3269 - val_accuracy: 0.8624\n",
      "Epoch 44/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8806\n",
      "Epoch 00044: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5816s 99ms/step - loss: 0.2783 - accuracy: 0.8806 - val_loss: 0.3255 - val_accuracy: 0.8616\n",
      "Epoch 45/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8810\n",
      "Epoch 00045: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5844s 100ms/step - loss: 0.2776 - accuracy: 0.8810 - val_loss: 0.3403 - val_accuracy: 0.8546\n",
      "Epoch 46/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2769 - accuracy: 0.8813\n",
      "Epoch 00046: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5827s 99ms/step - loss: 0.2769 - accuracy: 0.8813 - val_loss: 0.3371 - val_accuracy: 0.8580\n",
      "Epoch 47/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.8816\n",
      "Epoch 00047: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5839s 100ms/step - loss: 0.2764 - accuracy: 0.8816 - val_loss: 0.3208 - val_accuracy: 0.8648\n",
      "Epoch 48/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2757 - accuracy: 0.8819\n",
      "Epoch 00048: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5906s 101ms/step - loss: 0.2757 - accuracy: 0.8819 - val_loss: 0.3505 - val_accuracy: 0.8528\n",
      "Epoch 49/100\n",
      "58610/58611 [============================>.] - ETA: 0s - loss: 0.2750 - accuracy: 0.8821\n",
      "Epoch 00049: val_loss did not improve from 0.31421\n",
      "58611/58611 [==============================] - 5900s 101ms/step - loss: 0.2750 - accuracy: 0.8821 - val_loss: 0.3213 - val_accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,  \n",
    "      epochs=100,\n",
    "    validation_data = validation_generator,\n",
    "      verbose=1,\n",
    "      callbacks=[tensorboard_callback, checkpoint, earlyStopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30260), started 0:03:07 ago. (Use '!kill 30260' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7e1346108cb81649\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7e1346108cb81649\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
