{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow imports\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, MaxPooling2D, Dense, Input, Activation, AveragePooling2D, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, GlorotNormal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from scipy import signal\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "from matplotlib import pyplot as plt\n",
    "from pywt._doc_utils import wavedec2_keys, draw_2d_wp_basis\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a filter\n",
    "filter1 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [-1,-2,-3], [1,2,3]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "filtera1 = rotate(filter1, angle=90)\n",
    "filtera2 = rotate(filter1, angle=180)\n",
    "filtera3 = rotate(filter1, angle=270)\n",
    "\n",
    "\n",
    "# d filter\n",
    "filter2 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [-1,-2,-3],[2,4,6],[-1,-2,-3], [0,0,0]],\n",
    "                    [[0,0,0], [2,4,6],[-4,-8,-12], [2,4,6],[0,0,0]],\n",
    "                    [[0,0,0], [-1,-2,-3], [2,4,6], [-1,-2,-3], [0,0,0]],\n",
    "                    [[0,0,0],[0,0,0],[0,0,0], [0,0,0], [0,0,0]]])\n",
    "\n",
    "\n",
    "# b filter\n",
    "fitler3 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [1,2,3], [-2,-4,-6], [1,2,3], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "filterb1 =  signal.convolve(filter2, filter2, mode='same')\n",
    "filterb2 =  signal.convolve(filter2, filter2, mode='same')\n",
    "filterb3 =  signal.convolve(filter2, filter2, mode='same')\n",
    "\n",
    "# c filter\n",
    "filter4 = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]], [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]], [\n",
    "    [0,0,0], [1,2,3],[-3,-6,-9],[3,6,9],[-1,-2,-3]], [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                    [[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]]])\n",
    "\n",
    "#this is not filter\n",
    "filter4RDT = np.array([[[0,0,0], [0,0,0], [0,0,0], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [1,2,3], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [-3,-6,-9], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [3,6,9], [0,0,0], [0,0,0]],\n",
    "                       [[0,0,0], [0,0,0], [-1,-2,-3], [0,0,0], [0,0,0]]])\n",
    "\n",
    "\n",
    "filterc1 =  signal.convolve(filter4, filter4, mode='same')\n",
    "filterc2 =  signal.convolve(filter4.T, filter4.T, mode='same').reshape(5,5,3)\n",
    "filterc3 =  signal.convolve(filter4RDT, filter4RDT, mode='same').reshape(5,5,3)\n",
    "\n",
    "#e filter\n",
    "filter5 = np.array([[[-1,-2,-3], [2,4,6], [-2,-4,-6], [2,4,6], [-1,-2,-3]],\n",
    "                    [[2,4,6], [-6,-12,-18], [-8,-16,-24], [-6,-12,-18], [2,4,6]],\n",
    "                   [[-2,-4,-6], [8,16,24], [-12,-24,-36], [8,16,24], [2,4,6]],\n",
    "                    [[2,4,6], [-6,-12,-18], [8,16,24], [-6,-12,-18], [2,4,6]],\n",
    "                    [[-1,-2,-3], [2,4,6], [-2,-4,-6], [2,4,6], [-1,-2,-3]]])\n",
    "\n",
    "filter_list = [filter1.tolist(), filtera1.tolist(), filtera2.tolist(), filtera3.tolist(), filter2.tolist(),\\\n",
    "               fitler3.tolist(), filterb1.tolist(), filterb2.tolist(), filterb3.tolist(),\\\n",
    "               filter4.tolist(), filterc1.tolist(), filterc2.tolist(), filterc3.tolist(), filter5.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 5, 5, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([filter1,filtera1, filtera2, filtera3, filter2, fitler3, filterb1, filterb2, filterb3, filter4, filterc1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 14)        1064      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 16)        5616      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 62, 62, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 62, 62, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 62, 62, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 31, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "ouput (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 81,593\n",
      "Trainable params: 80,601\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inp = Input(shape = (64, 64, 3))\n",
    "x = Conv2D(14, (5,5), padding = 'same', name = 'conv2d_1')(Inp)\n",
    "x = Conv2D(16, (5,5), kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_2')(x)\n",
    "x = ZeroPadding2D((1,1))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(32, (5,5), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(64, (3,3), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(128, (1,1), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_5')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(256, (1,1), padding = 'same',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'conv2d_6')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('tanh')(x)\n",
    "x = AveragePooling2D((2,2))(x)\n",
    "x = Flatten()(x)\n",
    "out = Dense(1, activation = 'sigmoid',kernel_initializer= tf.keras.initializers.glorot_normal(seed=26), name = 'ouput')(x)\n",
    "\n",
    "model = Model(inputs = [Inp], outputs = [out])\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, show_shapes =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/logs')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/weights')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../models/checkpoints')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(\"../models/weights/tanh_history_log.csv\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "log_dir = r\"..\\\\models\\\\logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filepath = '../models/checkpoints/model-tanh-try1.h5';\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.get_layer('conv2d_1')\n",
    "layer.set_weights([np.array(filter_list).astype(np.float32).reshape(5,5,3,14), np.zeros(14,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1559902 images belonging to 2 classes.\n",
      "Found 222467 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.0,shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../input/patches/train/',  # This is the source directory for training images\n",
    "        target_size=(64, 64),  # All images will be resized to 64x64\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        '../input/patches/validation/',  # This is the source directory for validation images\n",
    "        target_size=(64, 64),  # All images will be resized to 64x64\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 48747 steps, validate for 6953 steps\n",
      "Epoch 1/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9248\n",
      "Epoch 00001: val_loss improved from inf to 0.23647, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2365s 49ms/step - loss: 0.2417 - accuracy: 0.9248 - val_loss: 0.2365 - val_accuracy: 0.9269\n",
      "Epoch 2/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9250\n",
      "Epoch 00002: val_loss improved from 0.23647 to 0.23599, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2323s 48ms/step - loss: 0.2350 - accuracy: 0.9250 - val_loss: 0.2360 - val_accuracy: 0.9277\n",
      "Epoch 3/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9252\n",
      "Epoch 00003: val_loss improved from 0.23599 to 0.23041, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2310s 47ms/step - loss: 0.2316 - accuracy: 0.9252 - val_loss: 0.2304 - val_accuracy: 0.9269\n",
      "Epoch 4/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9256\n",
      "Epoch 00004: val_loss improved from 0.23041 to 0.22610, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2317s 48ms/step - loss: 0.2278 - accuracy: 0.9256 - val_loss: 0.2261 - val_accuracy: 0.9283\n",
      "Epoch 5/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9264\n",
      "Epoch 00005: val_loss improved from 0.22610 to 0.22060, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2298s 47ms/step - loss: 0.2231 - accuracy: 0.9264 - val_loss: 0.2206 - val_accuracy: 0.9283\n",
      "Epoch 6/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9274\n",
      "Epoch 00006: val_loss improved from 0.22060 to 0.21203, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2284s 47ms/step - loss: 0.2170 - accuracy: 0.9274 - val_loss: 0.2120 - val_accuracy: 0.9290\n",
      "Epoch 7/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9285\n",
      "Epoch 00007: val_loss improved from 0.21203 to 0.20669, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2289s 47ms/step - loss: 0.2115 - accuracy: 0.9285 - val_loss: 0.2067 - val_accuracy: 0.9312\n",
      "Epoch 8/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.2069 - accuracy: 0.9294 ETA: 0s - loss: 0.2069 - accuracy: \n",
      "Epoch 00008: val_loss improved from 0.20669 to 0.20566, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2278s 47ms/step - loss: 0.2069 - accuracy: 0.9294 - val_loss: 0.2057 - val_accuracy: 0.9308\n",
      "Epoch 9/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9298\n",
      "Epoch 00009: val_loss improved from 0.20566 to 0.20453, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2284s 47ms/step - loss: 0.2028 - accuracy: 0.9298 - val_loss: 0.2045 - val_accuracy: 0.9296\n",
      "Epoch 10/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9307\n",
      "Epoch 00010: val_loss improved from 0.20453 to 0.20078, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2286s 47ms/step - loss: 0.1997 - accuracy: 0.9307 - val_loss: 0.2008 - val_accuracy: 0.9308\n",
      "Epoch 11/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9311\n",
      "Epoch 00011: val_loss improved from 0.20078 to 0.19983, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2291s 47ms/step - loss: 0.1966 - accuracy: 0.9311 - val_loss: 0.1998 - val_accuracy: 0.9295\n",
      "Epoch 12/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1938 - accuracy: 0.9317\n",
      "Epoch 00012: val_loss did not improve from 0.19983\n",
      "48747/48747 [==============================] - 2301s 47ms/step - loss: 0.1938 - accuracy: 0.9317 - val_loss: 0.2011 - val_accuracy: 0.9291\n",
      "Epoch 13/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9323\n",
      "Epoch 00013: val_loss did not improve from 0.19983\n",
      "48747/48747 [==============================] - 2284s 47ms/step - loss: 0.1911 - accuracy: 0.9323 - val_loss: 0.2066 - val_accuracy: 0.9282\n",
      "Epoch 14/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9328\n",
      "Epoch 00014: val_loss improved from 0.19983 to 0.19771, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2286s 47ms/step - loss: 0.1884 - accuracy: 0.9328 - val_loss: 0.1977 - val_accuracy: 0.9306\n",
      "Epoch 15/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9334\n",
      "Epoch 00015: val_loss did not improve from 0.19771\n",
      "48747/48747 [==============================] - 2277s 47ms/step - loss: 0.1863 - accuracy: 0.9334 - val_loss: 0.2171 - val_accuracy: 0.9251\n",
      "Epoch 16/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9339\n",
      "Epoch 00016: val_loss did not improve from 0.19771\n",
      "48747/48747 [==============================] - 2273s 47ms/step - loss: 0.1840 - accuracy: 0.9339 - val_loss: 0.1996 - val_accuracy: 0.9269\n",
      "Epoch 17/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9346\n",
      "Epoch 00017: val_loss did not improve from 0.19771\n",
      "48747/48747 [==============================] - 2270s 47ms/step - loss: 0.1814 - accuracy: 0.9346 - val_loss: 0.2028 - val_accuracy: 0.9249\n",
      "Epoch 18/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9350\n",
      "Epoch 00018: val_loss improved from 0.19771 to 0.19457, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2274s 47ms/step - loss: 0.1792 - accuracy: 0.9350 - val_loss: 0.1946 - val_accuracy: 0.9287\n",
      "Epoch 19/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9357\n",
      "Epoch 00019: val_loss did not improve from 0.19457\n",
      "48747/48747 [==============================] - 2286s 47ms/step - loss: 0.1773 - accuracy: 0.9357 - val_loss: 0.1979 - val_accuracy: 0.9291\n",
      "Epoch 20/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1751 - accuracy: 0.9362\n",
      "Epoch 00020: val_loss did not improve from 0.19457\n",
      "48747/48747 [==============================] - 2291s 47ms/step - loss: 0.1751 - accuracy: 0.9362 - val_loss: 0.1977 - val_accuracy: 0.9275\n",
      "Epoch 21/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9367\n",
      "Epoch 00021: val_loss improved from 0.19457 to 0.18709, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2292s 47ms/step - loss: 0.1732 - accuracy: 0.9367 - val_loss: 0.1871 - val_accuracy: 0.9326\n",
      "Epoch 22/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9373\n",
      "Epoch 00022: val_loss did not improve from 0.18709\n",
      "48747/48747 [==============================] - 2287s 47ms/step - loss: 0.1715 - accuracy: 0.9373 - val_loss: 0.1875 - val_accuracy: 0.9321\n",
      "Epoch 23/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9376\n",
      "Epoch 00023: val_loss did not improve from 0.18709\n",
      "48747/48747 [==============================] - 2274s 47ms/step - loss: 0.1698 - accuracy: 0.9376 - val_loss: 0.1873 - val_accuracy: 0.9314\n",
      "Epoch 24/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9380\n",
      "Epoch 00024: val_loss improved from 0.18709 to 0.18471, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2272s 47ms/step - loss: 0.1683 - accuracy: 0.9380 - val_loss: 0.1847 - val_accuracy: 0.9299\n",
      "Epoch 25/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9384\n",
      "Epoch 00025: val_loss did not improve from 0.18471\n",
      "48747/48747 [==============================] - 2269s 47ms/step - loss: 0.1667 - accuracy: 0.9384 - val_loss: 0.1877 - val_accuracy: 0.9301\n",
      "Epoch 26/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9390\n",
      "Epoch 00026: val_loss did not improve from 0.18471\n",
      "48747/48747 [==============================] - 2271s 47ms/step - loss: 0.1653 - accuracy: 0.9389 - val_loss: 0.1870 - val_accuracy: 0.9285\n",
      "Epoch 27/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9393\n",
      "Epoch 00027: val_loss did not improve from 0.18471\n",
      "48747/48747 [==============================] - 2268s 47ms/step - loss: 0.1640 - accuracy: 0.9393 - val_loss: 0.1857 - val_accuracy: 0.9310\n",
      "Epoch 28/100\n",
      "48746/48747 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9396\n",
      "Epoch 00028: val_loss did not improve from 0.18471\n",
      "48747/48747 [==============================] - 2294s 47ms/step - loss: 0.1628 - accuracy: 0.9396 - val_loss: 0.2034 - val_accuracy: 0.9233\n",
      "Epoch 29/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9398\n",
      "Epoch 00029: val_loss improved from 0.18471 to 0.18136, saving model to ../models/checkpoints/model-tanh-try1.h5\n",
      "48747/48747 [==============================] - 2292s 47ms/step - loss: 0.1618 - accuracy: 0.9398 - val_loss: 0.1814 - val_accuracy: 0.9328\n",
      "Epoch 30/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9402\n",
      "Epoch 00030: val_loss did not improve from 0.18136\n",
      "48747/48747 [==============================] - 2279s 47ms/step - loss: 0.1609 - accuracy: 0.9402 - val_loss: 0.1931 - val_accuracy: 0.9269\n",
      "Epoch 31/100\n",
      "48745/48747 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9407\n",
      "Epoch 00031: val_loss did not improve from 0.18136\n",
      "48747/48747 [==============================] - 2266s 46ms/step - loss: 0.1598 - accuracy: 0.9407 - val_loss: 0.1945 - val_accuracy: 0.9254\n",
      "Epoch 32/100\n",
      "16935/48747 [=========>....................] - ETA: 23:05 - loss: 0.1590 - accuracy: 0.9409"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,  \n",
    "      epochs=100,\n",
    "    validation_data = validation_generator,\n",
    "      verbose=1,\n",
    "      callbacks=[tensorboard_callback, checkpoint, earlyStopping, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
